{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import string\n",
    "import spacy\n",
    "import copy\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.insert(0, '..')\n",
    "from insert_whitespace import append_text\n",
    "from nltk import Tree\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from setup import *\n",
    "from logger import LOGGER\n",
    "\n",
    "path_sample = os.path.join(DATA_PATH, \"_sample_doc.json\")  # ->root/data/original/_sample_doc.json\n",
    "GVC_PARSING_FOLDER = os.path.join(os.getcwd())\n",
    "OUT_PATH = os.path.join(GVC_PARSING_FOLDER, OUTPUT_FOLDER_NAME)\n",
    "source_path = os.path.join(GVC_PARSING_FOLDER, GVC_FOLDER_NAME)\n",
    "result_path = os.path.join(OUT_PATH, 'test_parsing')\n",
    "\n",
    "CONTEXT_COMB_SPAN = 5\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# opens and loads the newsplease-format out of the json file: _sample_doc.json\n",
    "with open(path_sample, \"r\") as file:\n",
    "    newsplease_format = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 14:56:35,912@9908 INFO 242730985(13):<module>|: Reading gold.conll...\n",
      "100%|█████████▉| 181554/181555 [35:12<00:00, 85.95it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Converts the given dataset for a specified language into the desired format.\n",
    ":param paths: The paths desired to process (intra- & cross_intra annotations)\n",
    ":param result_path: the test_parsing folder for that language\n",
    ":param out_path: the output folder for that language\n",
    ":param language: the language to process\n",
    ":param nlp: the spacy model that fits the desired language\n",
    "\"\"\"\n",
    "path = source_path\n",
    "\n",
    "conll_df = pd.DataFrame(columns=[TOPIC_SUBTOPIC, DOC_ID, SENT_ID, TOKEN_ID, TOKEN, REFERENCE])\n",
    "\n",
    "LOGGER.info(\"Reading gold.conll...\")\n",
    "mention_identifiers = []\n",
    "prev_doc_id = \"placeholder\"\n",
    "doc_token_counter = int(0)\n",
    "doc_ids = []\n",
    "\n",
    "with open(os.path.join(path, 'gold.conll'), encoding=\"utf-8\") as f:\n",
    "    conll_str = f.read()\n",
    "    conll_lines = conll_str.split(\"\\n\")\n",
    "    for i, conll_line in tqdm(enumerate(conll_lines), total=len(conll_lines)):\n",
    "        if i+1 == len(conll_lines):\n",
    "            break\n",
    "        if \"#begin document\" in conll_line or \"#end document\" in conll_line:\n",
    "            continue\n",
    "        if conll_line.split(\"\\t\")[0].split(\".\")[1] == \"DCT\":\n",
    "            continue\n",
    "\n",
    "        if \"t\" in conll_line.split(\"\\t\")[0].split(\".\")[1]:\n",
    "            sent_id = 0\n",
    "        else:\n",
    "            sent_id = int(conll_line.split(\"\\t\")[0].split(\".\")[1][1:])\n",
    "\n",
    "        doc_id = conll_line.split(\"\\t\")[0].split(\".\")[0]\n",
    "        if doc_id != prev_doc_id:\n",
    "            doc_ids.append(doc_id)\n",
    "            prev_doc_id = doc_id\n",
    "            doc_token_counter = 0\n",
    "\n",
    "        if \"(\" in conll_line.split(\"\\t\")[3]:\n",
    "            # format: corefID_docID_sentID_token1ID\n",
    "            mention_identifiers.append(conll_line.split(\"\\t\")[3].replace('(','').replace(')','')+\"_\"+conll_line.split(\"\\t\")[0].split(\".\")[0]+\"_\"+str(sent_id)+\"_\"+str(conll_line.split(\"\\t\")[0].split(\".\")[2]))  \n",
    "\n",
    "        conll_df = pd.concat([conll_df, pd.DataFrame({\n",
    "            TOPIC_SUBTOPIC: conll_line.split(\"\\t\")[0].split(\".\")[0],\n",
    "            DOC_ID: conll_line.split(\"\\t\")[0].split(\".\")[0],\n",
    "            SENT_ID: sent_id,\n",
    "            TOKEN_ID: int(conll_line.split(\"\\t\")[0].split(\".\")[2]),\n",
    "            \"doc_token_id\": doc_token_counter,\n",
    "            TOKEN: conll_line.split(\"\\t\")[1],\n",
    "            \"title_text_id\": conll_line.split(\"\\t\")[2],\n",
    "            REFERENCE: conll_line.split(\"\\t\")[3]\n",
    "        }, index=[0])])\n",
    "\n",
    "        doc_token_counter = int(doc_token_counter + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 topic/subtopic_name                            doc_id  \\\n",
      "0   40b69cf630792394ef837aee6c959ece  40b69cf630792394ef837aee6c959ece   \n",
      "0   40b69cf630792394ef837aee6c959ece  40b69cf630792394ef837aee6c959ece   \n",
      "0   40b69cf630792394ef837aee6c959ece  40b69cf630792394ef837aee6c959ece   \n",
      "0   40b69cf630792394ef837aee6c959ece  40b69cf630792394ef837aee6c959ece   \n",
      "0   40b69cf630792394ef837aee6c959ece  40b69cf630792394ef837aee6c959ece   \n",
      "..                               ...                               ...   \n",
      "0   4e9255b6f2f5e4cf549f7c6f02b204d0  4e9255b6f2f5e4cf549f7c6f02b204d0   \n",
      "0   4e9255b6f2f5e4cf549f7c6f02b204d0  4e9255b6f2f5e4cf549f7c6f02b204d0   \n",
      "0   4e9255b6f2f5e4cf549f7c6f02b204d0  4e9255b6f2f5e4cf549f7c6f02b204d0   \n",
      "0   4e9255b6f2f5e4cf549f7c6f02b204d0  4e9255b6f2f5e4cf549f7c6f02b204d0   \n",
      "0   4e9255b6f2f5e4cf549f7c6f02b204d0  4e9255b6f2f5e4cf549f7c6f02b204d0   \n",
      "\n",
      "   sent_id token_id    token reference  doc_token_id title_text_id  \n",
      "0        0        1   Police         -           0.0         TITLE  \n",
      "0        0        2        :         -           1.0         TITLE  \n",
      "0        0        3   7-year         -           2.0         TITLE  \n",
      "0        0        4        -         -           3.0         TITLE  \n",
      "0        0        5      old         -           4.0         TITLE  \n",
      "..     ...      ...      ...       ...           ...           ...  \n",
      "0        4       20  outside         -          87.0          BODY  \n",
      "0        4       21      the         -          88.0          BODY  \n",
      "0        4       22     home         -          89.0          BODY  \n",
      "0        4       23        .         -          90.0          BODY  \n",
      "0        4       24  NEWLINE         -          91.0          BODY  \n",
      "\n",
      "[180014 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(conll_df.head(-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_punct = lambda l: sum([1 for x in l if x in string.punctuation])\n",
    "\n",
    "source_path = os.path.join(GVC_PARSING_FOLDER, GVC_FOLDER_NAME)\n",
    "result_path = os.path.join(OUT_PATH, 'test_parsing')\n",
    "out_path = os.path.join(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_nltk_tree(node):\n",
    "    \"\"\"\n",
    "        Converts a sentence to a visually helpful tree-structure output.\n",
    "        Can be used to double-check if a determined head is correct.\n",
    "    \"\"\"\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7298/7298 [04:41<00:00, 25.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# remake documents in newsplease format\n",
    "with open(path_sample, \"r\") as file:\n",
    "    newsplease_format = json.load(file)\n",
    "\n",
    "doc_files = {}\n",
    "\n",
    "for i, mention_identifier in tqdm(enumerate(mention_identifiers), total=len(mention_identifiers)):\n",
    "    doc_id = mention_identifier.split(\"_\")[1]\n",
    "    doc_conll = conll_df[conll_df[DOC_ID] == doc_id]\n",
    "    \n",
    "    doc_str = \"\"\n",
    "    for i, row in doc_conll[doc_conll[\"title_text_id\"] == \"BODY\"].iterrows():\n",
    "        doc_str, word_fixed, no_whitespace = append_text(doc_str, row[TOKEN])\n",
    "    title_str = \"\"\n",
    "    for i, row in doc_conll[doc_conll[\"title_text_id\"] == \"TITLE\"].iterrows():\n",
    "        title_str, word_fixed, no_whitespace = append_text(title_str, row[TOKEN])\n",
    "    \n",
    "\n",
    "    newsplease_custom = copy.copy(newsplease_format)\n",
    "\n",
    "    newsplease_custom[\"filename\"] = \"gold.conll\"\n",
    "    newsplease_custom[\"text\"] = doc_str\n",
    "    newsplease_custom[\"source_domain\"] = doc_id\n",
    "    newsplease_custom[\"language\"] = \"en\"\n",
    "    newsplease_custom[\"title\"] = title_str\n",
    "    if newsplease_custom[\"title\"][-1] not in string.punctuation:\n",
    "        newsplease_custom[\"title\"] += \".\"\n",
    "\n",
    "    doc_files[doc_id] = newsplease_custom\n",
    "    #if topic_name not in os.listdir(result_path):\n",
    "    #    os.mkdir(os.path.join(result_path, topic_name))\n",
    "\n",
    "    with open(os.path.join(result_path, newsplease_custom[\"source_domain\"] + \".json\"),\n",
    "                \"w\") as file:\n",
    "        json.dump(newsplease_custom, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 818/7298 [00:46<06:00, 17.96it/s]2022-09-24 15:37:17,015@9908 INFO 2124046802(72):<module>|: Mention with ID 2cc9b0377fbf945c7974c61e329b1599_BODY. needs manual review. Could not determine the mention head automatically \n",
      "(Exceeded max iterations). 1\n",
      " 18%|█▊        | 1345/7298 [01:15<05:16, 18.83it/s]2022-09-24 15:37:45,949@9908 INFO 2124046802(72):<module>|: Mention with ID b654dd9c20db958f7cba3b6cff58b7b2_blast through the driver's seat and into needs manual review. Could not determine the mention head automatically \n",
      "(Exceeded max iterations). 1\n",
      "2022-09-24 15:37:45,949@9908 INFO 2124046802(215):<module>|: Mention with ID b654dd9c20db958f7cba3b6cff58b7b2_blast through the driver's seat and into needs manual review. Could not determine the mention head automatically. 1\n",
      " 29%|██▉       | 2143/7298 [01:59<04:39, 18.45it/s]2022-09-24 15:38:29,926@9908 INFO 2124046802(72):<module>|: Mention with ID 25f5f30be9489f9c98a13f3d2e850a48_SHOOTING. needs manual review. Could not determine the mention head automatically \n",
      "(Exceeded max iterations). 1\n",
      " 65%|██████▌   | 4766/7298 [04:23<02:18, 18.35it/s]2022-09-24 15:40:54,216@9908 INFO 2124046802(72):<module>|: Mention with ID 8efc377c8be902ce0a6ac33f062ef934_bullet came through the second - floor bedroom wall, pierced needs manual review. Could not determine the mention head automatically \n",
      "(Exceeded max iterations). 1\n",
      "2022-09-24 15:40:54,217@9908 INFO 2124046802(215):<module>|: Mention with ID 8efc377c8be902ce0a6ac33f062ef934_bullet came through the second - floor bedroom wall, pierced needs manual review. Could not determine the mention head automatically. 1\n",
      " 95%|█████████▍| 6922/7298 [06:24<00:20, 18.26it/s]2022-09-24 15:42:54,613@9908 INFO 2124046802(72):<module>|: Mention with ID af848081e21ad60951fb3ee2e951488d_grave' condition needs manual review. Could not determine the mention head automatically \n",
      "(Exceeded max iterations). 1\n",
      "2022-09-24 15:42:54,615@9908 INFO 2124046802(215):<module>|: Mention with ID af848081e21ad60951fb3ee2e951488d_grave' condition needs manual review. Could not determine the mention head automatically. 1\n",
      "100%|██████████| 7298/7298 [06:45<00:00, 18.02it/s]\n"
     ]
    }
   ],
   "source": [
    "need_manual_review_mention_head = {}\n",
    "entity_mentions_local = []\n",
    "summary_df = pd.DataFrame(columns = [DOC_ID,COREF_CHAIN,DESCRIPTION,MENTION_TYPE,MENTION_FULL_TYPE,MENTION_ID,TOKENS_STR])\n",
    "topic_id = 0\n",
    "\n",
    "for i, mention_identifier in tqdm(enumerate(mention_identifiers), total = len(mention_identifiers)):\n",
    "    coref_id = mention_identifier.split(\"_\")[0]\n",
    "    if mention_identifier.split(\"_\")[1] != doc_id and i != 0:\n",
    "        topic_id = topic_id + 1\n",
    "    doc_id = mention_identifier.split(\"_\")[1]\n",
    "    sent_id = int(mention_identifier.split(\"_\")[2])\n",
    "    token1_id = int(mention_identifier.split(\"_\")[3])\n",
    "\n",
    "    sent_conll = conll_df[(conll_df[SENT_ID] == sent_id) & (conll_df[TOPIC_SUBTOPIC] == doc_id)]\n",
    "    sentence_str = \"\"\n",
    "    mention_tokenized = []\n",
    "    split_mention_text = []\n",
    "    mention_text = \"\"\n",
    "    currently_adding = False\n",
    "    whole_mention_added = False\n",
    "    for i, row in sent_conll.iterrows():\n",
    "        sentence_str, word_fixed, no_whitespace = append_text(sentence_str, row[TOKEN])\n",
    "        if coref_id in row[REFERENCE] and not whole_mention_added:\n",
    "            currently_adding = True\n",
    "            mention_tokenized.append(row[TOKEN_ID])\n",
    "            mention_text, word_fixed, no_whitespace = append_text(mention_text, row[TOKEN])\n",
    "            split_mention_text.append(row[TOKEN])\n",
    "        else:\n",
    "            if currently_adding == True:\n",
    "                whole_mention_added = True\n",
    "\n",
    "    doc = nlp(sentence_str)\n",
    "\n",
    "    # counting character up to the first character of the mention within the sentence\n",
    "    if len(split_mention_text) > 1:\n",
    "        first_char_of_mention = sentence_str.find(\n",
    "            split_mention_text[0] + \" \" + split_mention_text[\n",
    "                1])  # more accurate finding (reduce error if first word is occurring multiple times (i.e. \"the\")\n",
    "    else:\n",
    "        first_char_of_mention = sentence_str.find(split_mention_text[0])\n",
    "    # last character directly behind mention\n",
    "    last_char_of_mention = sentence_str.find(split_mention_text[-1], len(sentence_str[\n",
    "                                                                            :first_char_of_mention]) + len(\n",
    "        mention_text) - len(split_mention_text[-1])) + len(\n",
    "        split_mention_text[-1])\n",
    "    if last_char_of_mention == 0:  # last char can't be first char of string\n",
    "        # handle special case if the last punctuation is part of mention\n",
    "        last_char_of_mention = len(sentence_str)\n",
    "\n",
    "    token_str = mention_text\n",
    "    t_subt = doc_id\n",
    "    token_range = range(token1_id, token1_id+len(mention_tokenized)-1)\n",
    "\n",
    "    #print(coref_id)\n",
    "    #print(\"sent: \" + str(sent_id))\n",
    "    #print(mention_text)\n",
    "    #print(mention_tokenized)\n",
    "    #print(sentence_str)\n",
    "    #print(first_char_of_mention)\n",
    "    #print(last_char_of_mention)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        if counter > 50:  # an error must have occurred, so break and add to manual review\n",
    "            need_manual_review_mention_head[str(t_subt) + \"_\" + str(mention_text)[:10]] = {\n",
    "                \"mention_text\": mention_text,\n",
    "                \"sentence_str\": sentence_str,\n",
    "                \"mention_head\": \"unknown\",\n",
    "                \"mention_tokens_amount\": len(mention_tokenized),\n",
    "                \"tolerance\": tolerance\n",
    "            }\n",
    "            LOGGER.info(\n",
    "                f\"Mention with ID {str(t_subt)}_{str(mention_text)} needs manual review. Could not determine the mention head automatically \\n(Exceeded max iterations). {str(tolerance)}\")\n",
    "            break\n",
    "\n",
    "        if sentence_str[-1] not in \".!?\" or mention_text[-1] == \".\":\n",
    "            # if the sentence does not end with a \".\", we have to add one\n",
    "            # for the algorithm to understand the sentence.\n",
    "            # (this \".\" isn't represented in the output later)\n",
    "            sentence_str = sentence_str + \".\"\n",
    "        char_after_first_token = sentence_str[\n",
    "            first_char_of_mention + len(split_mention_text[0])]\n",
    "\n",
    "        if len(split_mention_text) < len(re.split(\" \", sentence_str[\n",
    "                                                        first_char_of_mention:last_char_of_mention])) + 1 and \\\n",
    "                (last_char_of_mention >= len(sentence_str) or\n",
    "                    sentence_str[last_char_of_mention] in string.punctuation or\n",
    "                    sentence_str[last_char_of_mention] == \" \") and \\\n",
    "                str(sentence_str[first_char_of_mention - 1]) in str(\n",
    "            string.punctuation + \" \") and \\\n",
    "                char_after_first_token in str(string.punctuation + \" \"):\n",
    "            # The end of the sentence was reached or the next character is a punctuation\n",
    "\n",
    "            processed_chars = 0\n",
    "            added_spaces = 0\n",
    "            mention_doc_ids = []\n",
    "\n",
    "            # get the tokens within the spacy doc\n",
    "            for t in doc:\n",
    "                processed_chars = processed_chars + len(t.text)\n",
    "                spaces = sentence_str[:processed_chars].count(\" \") - added_spaces\n",
    "                added_spaces = added_spaces + spaces\n",
    "                processed_chars = processed_chars + spaces\n",
    "\n",
    "                if last_char_of_mention >= processed_chars >= first_char_of_mention:\n",
    "                    # mention token detected\n",
    "                    mention_doc_ids.append(t.i)\n",
    "                elif processed_chars > last_char_of_mention:\n",
    "                    # whole mention has been processed\n",
    "                    break\n",
    "\n",
    "            # allow for dynamic differences in tokenization\n",
    "            # (longer mention texts may lead to more differences)\n",
    "            tolerance = 0\n",
    "            if tolerance > 2:\n",
    "                tolerance = 2\n",
    "            # tolerance for website mentions\n",
    "            if \".com\" in mention_text or \".org\" in mention_text:\n",
    "                tolerance = tolerance + 2\n",
    "            # tolerance when the mention has external tokens inbetween mention tokens\n",
    "            tolerance = tolerance \\\n",
    "                        + int(count_punct(token_str)) \\\n",
    "                        + 1\n",
    "            \n",
    "            #print(tolerance)\n",
    "\n",
    "            if abs(len(re.split(\" \", sentence_str[\n",
    "                                        first_char_of_mention:last_char_of_mention])) - len(\n",
    "                mention_tokenized)) <= tolerance and sentence_str[\n",
    "                first_char_of_mention - 1] in string.punctuation + \" \" and sentence_str[\n",
    "                last_char_of_mention] in string.punctuation + \" \":\n",
    "                # Whole mention found in sentence (and tolerance is OK)\n",
    "                break\n",
    "            else:\n",
    "                counter = counter + 1\n",
    "                # The next char is not a punctuation, so it therefore it is just a part of a bigger word\n",
    "                first_char_of_mention = sentence_str.find(\n",
    "                    re.split(\" \", mention_text)[0],\n",
    "                    first_char_of_mention + 2)\n",
    "                last_char_of_mention = sentence_str.find(\n",
    "                    re.split(\" \", mention_text)[-1],\n",
    "                    first_char_of_mention + len(\n",
    "                        re.split(\" \", mention_text)[0])) + len(\n",
    "                    re.split(\" \", mention_text)[-1])\n",
    "\n",
    "        else:\n",
    "            counter = counter + 1\n",
    "            # The next char is not a punctuation, so it therefore we just see a part of a bigger word\n",
    "            # i.g. do not accept \"her\" as a full word if the next letter is \"s\" (\"herself\")\n",
    "            first_char_of_mention = sentence_str.find(re.split(\" \", mention_text)[0],\n",
    "                                                        first_char_of_mention + 2)\n",
    "            if len(re.split(\" \", mention_text)) == 1:\n",
    "                last_char_of_mention = first_char_of_mention + len(mention_text)\n",
    "            else:\n",
    "                last_char_of_mention = sentence_str.find(re.split(\" \", mention_text)[-1],\n",
    "                                                            first_char_of_mention + len(\n",
    "                                                                re.split(\" \", mention_text)[\n",
    "                                                                    0])) + len(\n",
    "                    re.split(\" \", mention_text)[-1])\n",
    "\n",
    "    # whole mention string processed, look for the head\n",
    "    if str(t_subt) + \"_\" + str(mention_text) not in need_manual_review_mention_head:\n",
    "        for i in mention_doc_ids:\n",
    "            ancestors_in_mention = 0\n",
    "            for a in doc[i].ancestors:\n",
    "                if a.i in mention_doc_ids:\n",
    "                    ancestors_in_mention = ancestors_in_mention + 1\n",
    "                    break  # one is enough to make the token inviable as a head\n",
    "            if ancestors_in_mention == 0 and doc[i].text not in string.punctuation:  # puncts should not be heads\n",
    "                # head within the mention\n",
    "                mention_head = doc[i]\n",
    "    else:\n",
    "        mention_head = doc[0]  # as placeholder for manual checking\n",
    "\n",
    "    mention_head_lemma = mention_head.lemma_\n",
    "    mention_head_pos = mention_head.pos_\n",
    "\n",
    "    mention_ner = mention_head.ent_type_\n",
    "    if mention_ner == \"\":\n",
    "        mention_ner = \"O\"\n",
    "\n",
    "    # remap the mention head back to the meantime original tokenization to get the ID for the output\n",
    "    mention_head_id = None\n",
    "    mention_head_text = mention_head.text\n",
    "    for i,t in enumerate(split_mention_text):\n",
    "        if t.startswith(mention_head_text):\n",
    "            mention_head_id = mention_tokenized[i]\n",
    "\n",
    "    if not mention_head_id:\n",
    "        for i,t in enumerate(split_mention_text):\n",
    "            if mention_head_text.startswith(t):\n",
    "                mention_head_id = mention_tokenized[i]\n",
    "    if not mention_head_id:\n",
    "        for i,t in enumerate(split_mention_text):\n",
    "            if t.endswith(mention_head_text):\n",
    "                mention_head_id = mention_tokenized[i]\n",
    "\n",
    "    # add to manual review if the resulting token is not inside the mention\n",
    "    # (error must have happened)\n",
    "    if mention_head_id not in mention_tokenized:  # also \"if is None\"\n",
    "        if str(t_subt) + \"_\" + str(mention_text) not in need_manual_review_mention_head:\n",
    "            need_manual_review_mention_head[str(t_subt) + \"_\" + str(mention_text)[:10]] = \\\n",
    "                {\n",
    "                    \"mention_text\": mention_text,\n",
    "                    \"sentence_str\": sentence_str,\n",
    "                    \"mention_head\": str(mention_head),\n",
    "                    \"mention_tokens_amount\": len(mention_tokenized),\n",
    "                    \"tolerance\": tolerance\n",
    "                }\n",
    "            with open(os.path.join(out_path, MANUAL_REVIEW_FILE),\n",
    "                        \"w\",\n",
    "                        encoding='utf-8') as file:\n",
    "                json.dump(need_manual_review_mention_head, file)\n",
    "            #[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
    "            LOGGER.info(\n",
    "                f\"Mention with ID {str(t_subt)}_{str(mention_text)} needs manual review. Could not determine the mention head automatically. {str(tolerance)}\")\n",
    "\n",
    "    # get the context\n",
    "    context_min_id, context_max_id = [0 if int(min(mention_tokenized)) - CONTEXT_RANGE < 0 else\n",
    "                                        int(min(mention_tokenized)) - CONTEXT_RANGE,\n",
    "                                        int(max(mention_tokenized)) + CONTEXT_RANGE]\n",
    "\n",
    "    mention_context_str = []\n",
    "    break_indicator = False\n",
    "    # append to the mention context string list\n",
    "    '''\n",
    "    for sent in sentences:\n",
    "        sent_words = []\n",
    "        for s in sent.iter():\n",
    "            if s.tag == 'word':\n",
    "                sent_words.append(s)\n",
    "        for word in sent_words:\n",
    "            if word.get(\"wdid\") is None:\n",
    "                if len(mention_context_str) > 0:\n",
    "                    mention_context_str.append(word.get(\"wd\"))\n",
    "            elif int(word.get(\"wdid\")[1:]) > context_max_id:  # break when all needed words processed\n",
    "                break_indicator = True\n",
    "                break\n",
    "            elif int(word.get(\"wdid\")[1:]) >= context_min_id and int(word.get(\"wdid\")[1:]) <= context_max_id:\n",
    "                mention_context_str.append(word.get(\"wd\"))\n",
    "        if break_indicator is True:\n",
    "            break\n",
    "    '''\n",
    "    sent_words = []\n",
    "    for i, row in conll_df[conll_df[DOC_ID] == doc_id].iterrows():\n",
    "        if row[\"doc_token_id\"] > context_max_id:\n",
    "            break\n",
    "        elif context_min_id >= row[\"doc_token_id\"] >= context_min_id:\n",
    "            mention_context_str.append(row[TOKEN])\n",
    "\n",
    "    # add to mentions if the variables are correct ( do not add for manual review needed )\n",
    "    if str(t_subt) + \"_\" + str(mention_text) not in need_manual_review_mention_head:\n",
    "        mention = {COREF_CHAIN: coref_id,\n",
    "                    MENTION_NER: mention_ner,\n",
    "                    MENTION_HEAD_POS: mention_head_pos,\n",
    "                    MENTION_HEAD_LEMMA: mention_head_lemma,\n",
    "                    MENTION_HEAD: mention_head_text,\n",
    "                    MENTION_HEAD_ID: mention_head_id,\n",
    "                    DOC_ID: doc_id,\n",
    "                    DOC_ID_FULL: doc_id,\n",
    "                    IS_CONTINIOUS: mention_tokenized == list(range(mention_tokenized[0], mention_tokenized[-1] + 1)),\n",
    "                    IS_SINGLETON: len(mention_tokenized) == 1,\n",
    "                    MENTION_ID: mention_identifier,\n",
    "                    MENTION_TYPE: \"MIS\",\n",
    "                    MENTION_FULL_TYPE: \"MISC\",\n",
    "                    SCORE: -1.0,\n",
    "                    SENT_ID: sent_id,\n",
    "                    MENTION_CONTEXT: mention_context_str,\n",
    "                    TOKENS_NUMBER: mention_tokenized,\n",
    "                    TOKENS_STR: token_str,\n",
    "                    TOKENS_TEXT: split_mention_text,\n",
    "                    TOPIC_ID: topic_id,\n",
    "                    TOPIC: t_subt,\n",
    "                    SUBTOPIC: t_subt,\n",
    "                    TOPIC_SUBTOPIC: t_subt,\n",
    "                    COREF_TYPE: STRICT, #coref_type\n",
    "                    DESCRIPTION: None,\n",
    "                    CONLL_DOC_KEY: t_subt\n",
    "                    }\n",
    "\n",
    "        # only has entities\n",
    "        entity_mentions_local.append(mention)\n",
    "\n",
    "        summary_df.loc[len(summary_df)] = {\n",
    "            DOC_ID: doc_id,\n",
    "            COREF_CHAIN: coref_id,\n",
    "            DESCRIPTION: \"\",\n",
    "            MENTION_TYPE: \"MIS\",\n",
    "            MENTION_FULL_TYPE: \"MISC\",\n",
    "            MENTION_ID: None,\n",
    "            TOKENS_STR: token_str\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 15:43:15,501@9908 WARNING 1202988677(2):<module>|: Mentions ignored: 5. The ignored mentions are available here for a manual review: c:\\Users\\snake\\Documents\\GitHub\\Diverse_CDCR_datasets\\GVC-prep\\output_data\\manual_review_needed.json\n"
     ]
    }
   ],
   "source": [
    "if len(need_manual_review_mention_head):\n",
    "    LOGGER.warning(f'Mentions ignored: {len(need_manual_review_mention_head)}. The ignored mentions are available here for a manual review: '\n",
    "                f'{os.path.join(out_path,MANUAL_REVIEW_FILE)}')\n",
    "    with open(os.path.join(out_path, MANUAL_REVIEW_FILE), \"w\", encoding='utf-8') as file:\n",
    "        json.dump(need_manual_review_mention_head, file)\n",
    "\n",
    "with open(os.path.join(out_path, 'gvc.conll'), \"w\", encoding='utf-8') as file:\n",
    "    file.write(conll_str)\n",
    "\n",
    "with open(os.path.join(out_path, MENTIONS_ENTITIES_JSON), \"w\", encoding='utf-8') as file:\n",
    "    json.dump(entity_mentions_local, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('cdcr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "b7cf66addd738740e096863238041d528e5bd20d9f9be802df92f68260e86ac8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import string\n",
    "import spacy\n",
    "import copy\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.insert(0, '..')\n",
    "from insert_whitespace import append_text\n",
    "from nltk import Tree\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from setup import *\n",
    "from logger import LOGGER\n",
    "\n",
    "path_sample = os.path.join(DATA_PATH, \"_sample_doc.json\")  # ->root/data/original/_sample_doc.json\n",
    "GVC_PARSING_FOLDER = os.path.join(os.getcwd())\n",
    "OUT_PATH = os.path.join(GVC_PARSING_FOLDER, OUTPUT_FOLDER_NAME)\n",
    "source_path = os.path.join(GVC_PARSING_FOLDER, GVC_FOLDER_NAME)\n",
    "result_path = os.path.join(OUT_PATH, 'test_parsing')\n",
    "\n",
    "CONTEXT_COMB_SPAN = 5\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# opens and loads the newsplease-format out of the json file: _sample_doc.json\n",
    "with open(path_sample, \"r\") as file:\n",
    "    newsplease_format = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 19:21:34,871@18696 INFO 1565422891(13):<module>|: Reading verbose.conll...\n",
      "100%|█████████▉| 181554/181555 [26:54<00:00, 112.47it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Converts the given dataset for a specified language into the desired format.\n",
    ":param paths: The paths desired to process (intra- & cross_intra annotations)\n",
    ":param result_path: the test_parsing folder for that language\n",
    ":param out_path: the output folder for that language\n",
    ":param language: the language to process\n",
    ":param nlp: the spacy model that fits the desired language\n",
    "\"\"\"\n",
    "path = source_path\n",
    "\n",
    "conll_df = pd.DataFrame(columns=[TOPIC_SUBTOPIC, DOC_ID, SENT_ID, TOKEN_ID, \"doc_token_id\", TOKEN, \"title_text_id\", REFERENCE])\n",
    "\n",
    "LOGGER.info(\"Reading verbose.conll...\")\n",
    "mention_identifiers = []\n",
    "prev_doc_id = \"placeholder\"\n",
    "doc_token_counter = int(0)\n",
    "doc_ids = []\n",
    "\n",
    "with open(os.path.join(path, 'verbose.conll'), encoding=\"utf-8\") as f:\n",
    "    conll_str = f.read()\n",
    "    conll_lines = conll_str.split(\"\\n\")\n",
    "    for i, conll_line in tqdm(enumerate(conll_lines), total=len(conll_lines)):\n",
    "        if i+1 == len(conll_lines):\n",
    "            break\n",
    "        if \"#begin document\" in conll_line or \"#end document\" in conll_line:\n",
    "            continue\n",
    "        if conll_line.split(\"\\t\")[0].split(\".\")[1] == \"DCT\":\n",
    "            continue\n",
    "\n",
    "        if \"t\" in conll_line.split(\"\\t\")[0].split(\".\")[1]:\n",
    "            sent_id = 0\n",
    "        else:\n",
    "            sent_id = int(conll_line.split(\"\\t\")[0].split(\".\")[1][1:])\n",
    "\n",
    "        doc_id = conll_line.split(\"\\t\")[0].split(\".\")[0]\n",
    "        if doc_id != prev_doc_id:\n",
    "            doc_ids.append(doc_id)\n",
    "            prev_doc_id = doc_id\n",
    "            doc_token_counter = 0\n",
    "\n",
    "        if \"(\" in conll_line.split(\"\\t\")[4]:\n",
    "            # format: corefID_docID_sentID_token1ID\n",
    "            mention_identifiers.append(conll_line.split(\"\\t\")[4].replace('(','').replace(')','')+\"_\"+conll_line.split(\"\\t\")[0].split(\".\")[0]+\"_\"+str(sent_id)+\"_\"+str(conll_line.split(\"\\t\")[0].split(\".\")[2]))  \n",
    "\n",
    "        conll_df = pd.concat([conll_df, pd.DataFrame({\n",
    "            TOPIC_SUBTOPIC: conll_line.split(\"\\t\")[3].split(\".\")[0],\n",
    "            DOC_ID: conll_line.split(\"\\t\")[0].split(\".\")[0],\n",
    "            SENT_ID: sent_id,\n",
    "            TOKEN_ID: int(conll_line.split(\"\\t\")[0].split(\".\")[2]),\n",
    "            \"doc_token_id\": doc_token_counter,\n",
    "            TOKEN: conll_line.split(\"\\t\")[1],\n",
    "            \"title_text_id\": conll_line.split(\"\\t\")[2],\n",
    "            REFERENCE: conll_line.split(\"\\t\")[4]\n",
    "        }, index=[0])])\n",
    "\n",
    "        doc_token_counter = int(doc_token_counter + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(mention_identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_punct = lambda l: sum([1 for x in l if x in string.punctuation])\n",
    "\n",
    "source_path = os.path.join(GVC_PARSING_FOLDER, GVC_FOLDER_NAME)\n",
    "result_path = os.path.join(OUT_PATH, 'test_parsing')\n",
    "out_path = os.path.join(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_nltk_tree(node):\n",
    "    \"\"\"\n",
    "        Converts a sentence to a visually helpful tree-structure output.\n",
    "        Can be used to double-check if a determined head is correct.\n",
    "    \"\"\"\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7298/7298 [04:25<00:00, 27.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# remake documents in newsplease format\n",
    "with open(path_sample, \"r\") as file:\n",
    "    newsplease_format = json.load(file)\n",
    "\n",
    "doc_files = {}\n",
    "\n",
    "for i, mention_identifier in tqdm(enumerate(mention_identifiers), total=len(mention_identifiers)):\n",
    "    doc_id = mention_identifier.split(\"_\")[1]\n",
    "    #print(doc_id)\n",
    "    doc_conll = conll_df[conll_df[DOC_ID] == doc_id]\n",
    "    #print(doc_conll.head())\n",
    "    \n",
    "    doc_str = \"\"\n",
    "    for i, row in doc_conll[doc_conll[\"title_text_id\"] == \"BODY\"].iterrows():\n",
    "        doc_str, word_fixed, no_whitespace = append_text(doc_str, row[TOKEN])\n",
    "    title_str = \"\"\n",
    "    for i, row in doc_conll[doc_conll[\"title_text_id\"] == \"TITLE\"].iterrows():\n",
    "        title_str, word_fixed, no_whitespace = append_text(title_str, row[TOKEN])\n",
    "    \n",
    "\n",
    "    newsplease_custom = copy.copy(newsplease_format)\n",
    "\n",
    "    newsplease_custom[\"filename\"] = \"verbose.conll\"\n",
    "    newsplease_custom[\"text\"] = doc_str\n",
    "    newsplease_custom[\"source_domain\"] = doc_id\n",
    "    newsplease_custom[\"language\"] = \"en\"\n",
    "    newsplease_custom[\"title\"] = title_str\n",
    "    if newsplease_custom[\"title\"][-1] not in string.punctuation:\n",
    "        newsplease_custom[\"title\"] += \".\"\n",
    "\n",
    "    doc_files[doc_id] = newsplease_custom\n",
    "    #if topic_name not in os.listdir(result_path):\n",
    "    #    os.mkdir(os.path.join(result_path, topic_name))\n",
    "\n",
    "    with open(os.path.join(result_path, newsplease_custom[\"source_domain\"] + \".json\"),\n",
    "                \"w\") as file:\n",
    "        json.dump(newsplease_custom, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 818/7298 [00:54<07:10, 15.06it/s]2022-09-28 19:53:49,675@18696 INFO 1208040437(75):<module>|: Mention with ID 680890_BODY. needs manual review. Could not determine the mention head automatically \n",
      "(Exceeded max iterations). 1\n",
      " 18%|█▊        | 1344/7298 [01:29<06:21, 15.60it/s]2022-09-28 19:54:24,652@18696 INFO 1208040437(75):<module>|: Mention with ID 226600_blast through the driver's seat and into needs manual review. Could not determine the mention head automatically \n",
      "(Exceeded max iterations). 1\n",
      "2022-09-28 19:54:24,653@18696 INFO 1208040437(218):<module>|: Mention with ID 226600_blast through the driver's seat and into needs manual review. Could not determine the mention head automatically. 1\n",
      " 29%|██▉       | 2144/7298 [02:23<05:41, 15.08it/s]2022-09-28 19:55:17,884@18696 INFO 1208040437(75):<module>|: Mention with ID 341726_SHOOTING. needs manual review. Could not determine the mention head automatically \n",
      "(Exceeded max iterations). 1\n",
      " 65%|██████▌   | 4766/7298 [05:17<02:44, 15.38it/s]2022-09-28 19:58:12,679@18696 INFO 1208040437(75):<module>|: Mention with ID 554759_bullet came through the second - floor bedroom wall, pierced needs manual review. Could not determine the mention head automatically \n",
      "(Exceeded max iterations). 1\n",
      "2022-09-28 19:58:12,680@18696 INFO 1208040437(218):<module>|: Mention with ID 554759_bullet came through the second - floor bedroom wall, pierced needs manual review. Could not determine the mention head automatically. 1\n",
      " 95%|█████████▍| 6922/7298 [07:41<00:24, 15.13it/s]2022-09-28 20:00:35,957@18696 INFO 1208040437(75):<module>|: Mention with ID 306048_grave' condition needs manual review. Could not determine the mention head automatically \n",
      "(Exceeded max iterations). 1\n",
      "2022-09-28 20:00:35,958@18696 INFO 1208040437(218):<module>|: Mention with ID 306048_grave' condition needs manual review. Could not determine the mention head automatically. 1\n",
      "100%|██████████| 7298/7298 [08:06<00:00, 15.01it/s]\n"
     ]
    }
   ],
   "source": [
    "need_manual_review_mention_head = {}\n",
    "entity_mentions_local = []\n",
    "summary_df = pd.DataFrame(columns = [DOC_ID,COREF_CHAIN,DESCRIPTION,MENTION_TYPE,MENTION_FULL_TYPE,MENTION_ID,TOKENS_STR])\n",
    "topic_id = 0\n",
    "\n",
    "for i, mention_identifier in tqdm(enumerate(mention_identifiers), total = len(mention_identifiers)):\n",
    "    coref_id = mention_identifier.split(\"_\")[0]\n",
    "    if mention_identifier.split(\"_\")[1] != doc_id and i != 0:\n",
    "        topic_id = topic_id + 1\n",
    "    doc_id = mention_identifier.split(\"_\")[1]\n",
    "    sent_id = int(mention_identifier.split(\"_\")[2])\n",
    "    token1_id = int(mention_identifier.split(\"_\")[3])\n",
    "\n",
    "    sent_conll = conll_df[(conll_df[SENT_ID] == sent_id) & (conll_df[DOC_ID] == doc_id)]\n",
    "    \n",
    "    sentence_str = \"\"\n",
    "    mention_tokenized = []\n",
    "    split_mention_text = []\n",
    "    mention_text = \"\"\n",
    "    currently_adding = False\n",
    "    whole_mention_added = False\n",
    "    for i, row in sent_conll.iterrows():\n",
    "        sentence_str, word_fixed, no_whitespace = append_text(sentence_str, row[TOKEN])\n",
    "        if coref_id in row[REFERENCE] and not whole_mention_added:\n",
    "            currently_adding = True\n",
    "            mention_tokenized.append(row[TOKEN_ID])\n",
    "            mention_text, word_fixed, no_whitespace = append_text(mention_text, row[TOKEN])\n",
    "            split_mention_text.append(row[TOKEN])\n",
    "        else:\n",
    "            if currently_adding == True:\n",
    "                whole_mention_added = True\n",
    "\n",
    "    doc = nlp(sentence_str)\n",
    "\n",
    "    # counting character up to the first character of the mention within the sentence\n",
    "    if len(split_mention_text) > 1:\n",
    "        first_char_of_mention = sentence_str.find(\n",
    "            split_mention_text[0] + \" \" + split_mention_text[\n",
    "                1])  # more accurate finding (reduce error if first word is occurring multiple times (i.e. \"the\")\n",
    "    else:\n",
    "        first_char_of_mention = sentence_str.find(split_mention_text[0])\n",
    "    # last character directly behind mention\n",
    "    last_char_of_mention = sentence_str.find(split_mention_text[-1], len(sentence_str[\n",
    "                                                                            :first_char_of_mention]) + len(\n",
    "        mention_text) - len(split_mention_text[-1])) + len(\n",
    "        split_mention_text[-1])\n",
    "    if last_char_of_mention == 0:  # last char can't be first char of string\n",
    "        # handle special case if the last punctuation is part of mention\n",
    "        last_char_of_mention = len(sentence_str)\n",
    "\n",
    "    token_str = mention_text\n",
    "    t_subt = sent_conll[sent_conll[TOPIC_SUBTOPIC] != \"-\"].iloc[0][TOPIC_SUBTOPIC]\n",
    "    #print(t_subt)\n",
    "    conll_df.loc[(conll_df[DOC_ID] == doc_id), TOPIC_SUBTOPIC]=t_subt    #  set subtopics in conll\n",
    "    token_range = range(token1_id, token1_id+len(mention_tokenized)-1)\n",
    "\n",
    "    #print(coref_id)\n",
    "    #print(\"sent: \" + str(sent_id))\n",
    "    #print(mention_text)\n",
    "    #print(mention_tokenized)\n",
    "    #print(sentence_str)\n",
    "    #print(first_char_of_mention)\n",
    "    #print(last_char_of_mention)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        if counter > 50:  # an error must have occurred, so break and add to manual review\n",
    "            need_manual_review_mention_head[str(t_subt) + \"_\" + str(mention_text)[:10]] = {\n",
    "                \"mention_text\": mention_text,\n",
    "                \"sentence_str\": sentence_str,\n",
    "                \"mention_head\": \"unknown\",\n",
    "                \"mention_tokens_amount\": len(mention_tokenized),\n",
    "                \"tolerance\": tolerance\n",
    "            }\n",
    "            LOGGER.info(\n",
    "                f\"Mention with ID {str(t_subt)}_{str(mention_text)} needs manual review. Could not determine the mention head automatically \\n(Exceeded max iterations). {str(tolerance)}\")\n",
    "            break\n",
    "\n",
    "        if sentence_str[-1] not in \".!?\" or mention_text[-1] == \".\":\n",
    "            # if the sentence does not end with a \".\", we have to add one\n",
    "            # for the algorithm to understand the sentence.\n",
    "            # (this \".\" isn't represented in the output later)\n",
    "            sentence_str = sentence_str + \".\"\n",
    "        char_after_first_token = sentence_str[\n",
    "            first_char_of_mention + len(split_mention_text[0])]\n",
    "\n",
    "        if len(split_mention_text) < len(re.split(\" \", sentence_str[\n",
    "                                                        first_char_of_mention:last_char_of_mention])) + 1 and \\\n",
    "                (last_char_of_mention >= len(sentence_str) or\n",
    "                    sentence_str[last_char_of_mention] in string.punctuation or\n",
    "                    sentence_str[last_char_of_mention] == \" \") and \\\n",
    "                str(sentence_str[first_char_of_mention - 1]) in str(\n",
    "            string.punctuation + \" \") and \\\n",
    "                char_after_first_token in str(string.punctuation + \" \"):\n",
    "            # The end of the sentence was reached or the next character is a punctuation\n",
    "\n",
    "            processed_chars = 0\n",
    "            added_spaces = 0\n",
    "            mention_doc_ids = []\n",
    "\n",
    "            # get the tokens within the spacy doc\n",
    "            for t in doc:\n",
    "                processed_chars = processed_chars + len(t.text)\n",
    "                spaces = sentence_str[:processed_chars].count(\" \") - added_spaces\n",
    "                added_spaces = added_spaces + spaces\n",
    "                processed_chars = processed_chars + spaces\n",
    "\n",
    "                if last_char_of_mention >= processed_chars >= first_char_of_mention:\n",
    "                    # mention token detected\n",
    "                    mention_doc_ids.append(t.i)\n",
    "                elif processed_chars > last_char_of_mention:\n",
    "                    # whole mention has been processed\n",
    "                    break\n",
    "\n",
    "            # allow for dynamic differences in tokenization\n",
    "            # (longer mention texts may lead to more differences)\n",
    "            tolerance = 0\n",
    "            if tolerance > 2:\n",
    "                tolerance = 2\n",
    "            # tolerance for website mentions\n",
    "            if \".com\" in mention_text or \".org\" in mention_text:\n",
    "                tolerance = tolerance + 2\n",
    "            # tolerance when the mention has external tokens inbetween mention tokens\n",
    "            tolerance = tolerance \\\n",
    "                        + int(count_punct(token_str)) \\\n",
    "                        + 1\n",
    "            \n",
    "            #print(tolerance)\n",
    "\n",
    "            if abs(len(re.split(\" \", sentence_str[\n",
    "                                        first_char_of_mention:last_char_of_mention])) - len(\n",
    "                mention_tokenized)) <= tolerance and sentence_str[\n",
    "                first_char_of_mention - 1] in string.punctuation + \" \" and sentence_str[\n",
    "                last_char_of_mention] in string.punctuation + \" \":\n",
    "                # Whole mention found in sentence (and tolerance is OK)\n",
    "                break\n",
    "            else:\n",
    "                counter = counter + 1\n",
    "                # The next char is not a punctuation, so it therefore it is just a part of a bigger word\n",
    "                first_char_of_mention = sentence_str.find(\n",
    "                    re.split(\" \", mention_text)[0],\n",
    "                    first_char_of_mention + 2)\n",
    "                last_char_of_mention = sentence_str.find(\n",
    "                    re.split(\" \", mention_text)[-1],\n",
    "                    first_char_of_mention + len(\n",
    "                        re.split(\" \", mention_text)[0])) + len(\n",
    "                    re.split(\" \", mention_text)[-1])\n",
    "\n",
    "        else:\n",
    "            counter = counter + 1\n",
    "            # The next char is not a punctuation, so it therefore we just see a part of a bigger word\n",
    "            # i.g. do not accept \"her\" as a full word if the next letter is \"s\" (\"herself\")\n",
    "            first_char_of_mention = sentence_str.find(re.split(\" \", mention_text)[0],\n",
    "                                                        first_char_of_mention + 2)\n",
    "            if len(re.split(\" \", mention_text)) == 1:\n",
    "                last_char_of_mention = first_char_of_mention + len(mention_text)\n",
    "            else:\n",
    "                last_char_of_mention = sentence_str.find(re.split(\" \", mention_text)[-1],\n",
    "                                                            first_char_of_mention + len(\n",
    "                                                                re.split(\" \", mention_text)[\n",
    "                                                                    0])) + len(\n",
    "                    re.split(\" \", mention_text)[-1])\n",
    "\n",
    "    # whole mention string processed, look for the head\n",
    "    if str(t_subt) + \"_\" + str(mention_text) not in need_manual_review_mention_head:\n",
    "        for i in mention_doc_ids:\n",
    "            ancestors_in_mention = 0\n",
    "            for a in doc[i].ancestors:\n",
    "                if a.i in mention_doc_ids:\n",
    "                    ancestors_in_mention = ancestors_in_mention + 1\n",
    "                    break  # one is enough to make the token inviable as a head\n",
    "            if ancestors_in_mention == 0 and doc[i].text not in string.punctuation:  # puncts should not be heads\n",
    "                # head within the mention\n",
    "                mention_head = doc[i]\n",
    "    else:\n",
    "        mention_head = doc[0]  # as placeholder for manual checking\n",
    "\n",
    "    mention_head_lemma = mention_head.lemma_\n",
    "    mention_head_pos = mention_head.pos_\n",
    "\n",
    "    mention_ner = mention_head.ent_type_\n",
    "    if mention_ner == \"\":\n",
    "        mention_ner = \"O\"\n",
    "\n",
    "    # remap the mention head back to the meantime original tokenization to get the ID for the output\n",
    "    mention_head_id = None\n",
    "    mention_head_text = mention_head.text\n",
    "    for i,t in enumerate(split_mention_text):\n",
    "        if t.startswith(mention_head_text):\n",
    "            mention_head_id = mention_tokenized[i]\n",
    "\n",
    "    if not mention_head_id:\n",
    "        for i,t in enumerate(split_mention_text):\n",
    "            if mention_head_text.startswith(t):\n",
    "                mention_head_id = mention_tokenized[i]\n",
    "    if not mention_head_id:\n",
    "        for i,t in enumerate(split_mention_text):\n",
    "            if t.endswith(mention_head_text):\n",
    "                mention_head_id = mention_tokenized[i]\n",
    "\n",
    "    # add to manual review if the resulting token is not inside the mention\n",
    "    # (error must have happened)\n",
    "    if mention_head_id not in mention_tokenized:  # also \"if is None\"\n",
    "        if str(t_subt) + \"_\" + str(mention_text) not in need_manual_review_mention_head:\n",
    "            need_manual_review_mention_head[str(t_subt) + \"_\" + str(mention_text)[:10]] = \\\n",
    "                {\n",
    "                    \"mention_text\": mention_text,\n",
    "                    \"sentence_str\": sentence_str,\n",
    "                    \"mention_head\": str(mention_head),\n",
    "                    \"mention_tokens_amount\": len(mention_tokenized),\n",
    "                    \"tolerance\": tolerance\n",
    "                }\n",
    "            with open(os.path.join(out_path, MANUAL_REVIEW_FILE),\n",
    "                        \"w\",\n",
    "                        encoding='utf-8') as file:\n",
    "                json.dump(need_manual_review_mention_head, file)\n",
    "            #[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
    "            LOGGER.info(\n",
    "                f\"Mention with ID {str(t_subt)}_{str(mention_text)} needs manual review. Could not determine the mention head automatically. {str(tolerance)}\")\n",
    "\n",
    "    # get the context\n",
    "    context_min_id, context_max_id = [0 if int(min(mention_tokenized)) - CONTEXT_RANGE < 0 else\n",
    "                                        int(min(mention_tokenized)) - CONTEXT_RANGE,\n",
    "                                        int(max(mention_tokenized)) + CONTEXT_RANGE]\n",
    "\n",
    "    mention_context_str = []\n",
    "    break_indicator = False\n",
    "    # append to the mention context string list\n",
    "    '''\n",
    "    for sent in sentences:\n",
    "        sent_words = []\n",
    "        for s in sent.iter():\n",
    "            if s.tag == 'word':\n",
    "                sent_words.append(s)\n",
    "        for word in sent_words:\n",
    "            if word.get(\"wdid\") is None:\n",
    "                if len(mention_context_str) > 0:\n",
    "                    mention_context_str.append(word.get(\"wd\"))\n",
    "            elif int(word.get(\"wdid\")[1:]) > context_max_id:  # break when all needed words processed\n",
    "                break_indicator = True\n",
    "                break\n",
    "            elif int(word.get(\"wdid\")[1:]) >= context_min_id and int(word.get(\"wdid\")[1:]) <= context_max_id:\n",
    "                mention_context_str.append(word.get(\"wd\"))\n",
    "        if break_indicator is True:\n",
    "            break\n",
    "    '''\n",
    "    sent_words = []\n",
    "    for i, row in conll_df[conll_df[DOC_ID] == doc_id].iterrows():\n",
    "        if row[\"doc_token_id\"] > context_max_id:\n",
    "            break\n",
    "        elif context_min_id >= row[\"doc_token_id\"] >= context_min_id:\n",
    "            mention_context_str.append(row[TOKEN])\n",
    "\n",
    "    # add to mentions if the variables are correct ( do not add for manual review needed )\n",
    "    if str(t_subt) + \"_\" + str(mention_text) not in need_manual_review_mention_head:\n",
    "        mention = {COREF_CHAIN: coref_id,\n",
    "                    MENTION_NER: mention_ner,\n",
    "                    MENTION_HEAD_POS: mention_head_pos,\n",
    "                    MENTION_HEAD_LEMMA: mention_head_lemma,\n",
    "                    MENTION_HEAD: mention_head_text,\n",
    "                    MENTION_HEAD_ID: mention_head_id,\n",
    "                    DOC_ID: doc_id,\n",
    "                    DOC_ID_FULL: doc_id,\n",
    "                    IS_CONTINIOUS: mention_tokenized == list(range(mention_tokenized[0], mention_tokenized[-1] + 1)),\n",
    "                    IS_SINGLETON: len(mention_tokenized) == 1,\n",
    "                    MENTION_ID: mention_identifier,\n",
    "                    MENTION_TYPE: \"MIS\",\n",
    "                    MENTION_FULL_TYPE: \"MISC\",\n",
    "                    SCORE: -1.0,\n",
    "                    SENT_ID: sent_id,\n",
    "                    MENTION_CONTEXT: mention_context_str,\n",
    "                    TOKENS_NUMBER: mention_tokenized,\n",
    "                    TOKENS_STR: token_str,\n",
    "                    TOKENS_TEXT: split_mention_text,\n",
    "                    TOPIC_ID: topic_id,\n",
    "                    TOPIC: \"-\",\n",
    "                    SUBTOPIC: t_subt,\n",
    "                    TOPIC_SUBTOPIC: t_subt,\n",
    "                    COREF_TYPE: STRICT, #coref_type\n",
    "                    DESCRIPTION: None,\n",
    "                    CONLL_DOC_KEY: t_subt\n",
    "                    }\n",
    "\n",
    "        # only has entities\n",
    "        entity_mentions_local.append(mention)\n",
    "\n",
    "        summary_df.loc[len(summary_df)] = {\n",
    "            DOC_ID: doc_id,\n",
    "            COREF_CHAIN: coref_id,\n",
    "            DESCRIPTION: \"\",\n",
    "            MENTION_TYPE: \"MIS\",\n",
    "            MENTION_FULL_TYPE: \"MISC\",\n",
    "            MENTION_ID: mention_identifier,\n",
    "            TOKENS_STR: token_str\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 20:01:01,032@18696 WARNING 898622286(2):<module>|: Mentions ignored: 5. The ignored mentions are available here for a manual review: c:\\Users\\snake\\Documents\\GitHub\\Diverse_CDCR_datasets\\GVC-prep\\output_data\\manual_review_needed.json\n"
     ]
    }
   ],
   "source": [
    "if len(need_manual_review_mention_head):\n",
    "    LOGGER.warning(f'Mentions ignored: {len(need_manual_review_mention_head)}. The ignored mentions are available here for a manual review: '\n",
    "                f'{os.path.join(out_path,MANUAL_REVIEW_FILE)}')\n",
    "    with open(os.path.join(out_path, MANUAL_REVIEW_FILE), \"w\", encoding='utf-8') as file:\n",
    "        json.dump(need_manual_review_mention_head, file)\n",
    "\n",
    "with open(os.path.join(out_path, MENTIONS_ENTITIES_JSON), \"w\", encoding='utf-8') as file:\n",
    "    json.dump(entity_mentions_local, file)\n",
    "\n",
    "summary_df.drop(columns=[MENTION_ID], inplace=True)\n",
    "summary_df.to_csv(os.path.join(out_path, MENTIONS_ALL_CSV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic/subtopic_name</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>doc_token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>title_text_id</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Police</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7-year</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>old</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>girl</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>dies</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>(29997179479991009991)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>after</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>shooting</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>(2999717947999115999857875)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>in</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>Salisbury</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>BODY</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>7-year</td>\n",
       "      <td>BODY</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>-</td>\n",
       "      <td>BODY</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>old</td>\n",
       "      <td>BODY</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>girl</td>\n",
       "      <td>BODY</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>was</td>\n",
       "      <td>BODY</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>killed</td>\n",
       "      <td>BODY</td>\n",
       "      <td>(29997179479991009991)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>early</td>\n",
       "      <td>BODY</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717947</td>\n",
       "      <td>40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>BODY</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  topic/subtopic_name                            doc_id sent_id token_id  \\\n",
       "0              717947  40b69cf630792394ef837aee6c959ece       0        1   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       0        2   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       0        3   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       0        4   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       0        5   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       0        6   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       0        7   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       0        8   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       0        9   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       0       10   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       0       11   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       1        1   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       1        2   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       1        3   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       1        4   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       1        5   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       1        6   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       1        7   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       1        8   \n",
       "0              717947  40b69cf630792394ef837aee6c959ece       1        9   \n",
       "\n",
       "  doc_token_id      token title_text_id                    reference  \n",
       "0            0     Police         TITLE                            -  \n",
       "0            1          :         TITLE                            -  \n",
       "0            2     7-year         TITLE                            -  \n",
       "0            3          -         TITLE                            -  \n",
       "0            4        old         TITLE                            -  \n",
       "0            5       girl         TITLE                            -  \n",
       "0            6       dies         TITLE       (29997179479991009991)  \n",
       "0            7      after         TITLE                            -  \n",
       "0            8   shooting         TITLE  (2999717947999115999857875)  \n",
       "0            9         in         TITLE                            -  \n",
       "0           10  Salisbury         TITLE                            -  \n",
       "0           11          A          BODY                            -  \n",
       "0           12     7-year          BODY                            -  \n",
       "0           13          -          BODY                            -  \n",
       "0           14        old          BODY                            -  \n",
       "0           15       girl          BODY                            -  \n",
       "0           16        was          BODY                            -  \n",
       "0           17     killed          BODY       (29997179479991009991)  \n",
       "0           18      early          BODY                            -  \n",
       "0           19     Sunday          BODY                            -  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 20:01:01,553@18696 INFO 4103975627(5):<module>|: Generating conll string...\n",
      "100%|██████████| 180024/180024 [00:29<00:00, 6117.17it/s]\n"
     ]
    }
   ],
   "source": [
    "conll_df = conll_df.reset_index(drop=True)\n",
    "final_output_str = \"\"\n",
    "\n",
    "# create a conll string from the conll_df\n",
    "LOGGER.info(\"Generating conll string...\")\n",
    "for i, row in tqdm(conll_df.iterrows(), total=conll_df.shape[0]):\n",
    "    conll_df.iloc[i][TOPIC_SUBTOPIC] = \"-/\" + row[TOPIC_SUBTOPIC] + \"/\" + row[DOC_ID]\n",
    "    if not \"(\" in row[REFERENCE] and not \")\" in row[REFERENCE]: \n",
    "        conll_df.iloc[i][REFERENCE] = \"-\"\n",
    "\n",
    "conll_df = conll_df.drop(columns=[DOC_ID, \"doc_token_id\", \"title_text_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic/subtopic_name</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Police</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>:</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7-year</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>old</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>girl</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>dies</td>\n",
       "      <td>(29997179479991009991)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>after</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>shooting</td>\n",
       "      <td>(2999717947999115999857875)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>in</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>Salisbury</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7-year</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>old</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>girl</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>was</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>killed</td>\n",
       "      <td>(29997179479991009991)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>early</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>morning</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>after</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>a</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>Salisbury</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>home</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>was</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>sprayed</td>\n",
       "      <td>(2999717947999115999857875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>with</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>bullets</td>\n",
       "      <td>2999717947999115999857875)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-/717947/40b69cf630792394ef837aee6c959ece</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>,</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          topic/subtopic_name sent_id token_id      token  \\\n",
       "0   -/717947/40b69cf630792394ef837aee6c959ece       0        1     Police   \n",
       "1   -/717947/40b69cf630792394ef837aee6c959ece       0        2          :   \n",
       "2   -/717947/40b69cf630792394ef837aee6c959ece       0        3     7-year   \n",
       "3   -/717947/40b69cf630792394ef837aee6c959ece       0        4          -   \n",
       "4   -/717947/40b69cf630792394ef837aee6c959ece       0        5        old   \n",
       "5   -/717947/40b69cf630792394ef837aee6c959ece       0        6       girl   \n",
       "6   -/717947/40b69cf630792394ef837aee6c959ece       0        7       dies   \n",
       "7   -/717947/40b69cf630792394ef837aee6c959ece       0        8      after   \n",
       "8   -/717947/40b69cf630792394ef837aee6c959ece       0        9   shooting   \n",
       "9   -/717947/40b69cf630792394ef837aee6c959ece       0       10         in   \n",
       "10  -/717947/40b69cf630792394ef837aee6c959ece       0       11  Salisbury   \n",
       "11  -/717947/40b69cf630792394ef837aee6c959ece       1        1          A   \n",
       "12  -/717947/40b69cf630792394ef837aee6c959ece       1        2     7-year   \n",
       "13  -/717947/40b69cf630792394ef837aee6c959ece       1        3          -   \n",
       "14  -/717947/40b69cf630792394ef837aee6c959ece       1        4        old   \n",
       "15  -/717947/40b69cf630792394ef837aee6c959ece       1        5       girl   \n",
       "16  -/717947/40b69cf630792394ef837aee6c959ece       1        6        was   \n",
       "17  -/717947/40b69cf630792394ef837aee6c959ece       1        7     killed   \n",
       "18  -/717947/40b69cf630792394ef837aee6c959ece       1        8      early   \n",
       "19  -/717947/40b69cf630792394ef837aee6c959ece       1        9     Sunday   \n",
       "20  -/717947/40b69cf630792394ef837aee6c959ece       1       10    morning   \n",
       "21  -/717947/40b69cf630792394ef837aee6c959ece       1       11      after   \n",
       "22  -/717947/40b69cf630792394ef837aee6c959ece       1       12          a   \n",
       "23  -/717947/40b69cf630792394ef837aee6c959ece       1       13  Salisbury   \n",
       "24  -/717947/40b69cf630792394ef837aee6c959ece       1       14       home   \n",
       "25  -/717947/40b69cf630792394ef837aee6c959ece       1       15        was   \n",
       "26  -/717947/40b69cf630792394ef837aee6c959ece       1       16    sprayed   \n",
       "27  -/717947/40b69cf630792394ef837aee6c959ece       1       17       with   \n",
       "28  -/717947/40b69cf630792394ef837aee6c959ece       1       18    bullets   \n",
       "29  -/717947/40b69cf630792394ef837aee6c959ece       1       19          ,   \n",
       "\n",
       "                      reference  \n",
       "0                             -  \n",
       "1                             -  \n",
       "2                             -  \n",
       "3                             -  \n",
       "4                             -  \n",
       "5                             -  \n",
       "6        (29997179479991009991)  \n",
       "7                             -  \n",
       "8   (2999717947999115999857875)  \n",
       "9                             -  \n",
       "10                            -  \n",
       "11                            -  \n",
       "12                            -  \n",
       "13                            -  \n",
       "14                            -  \n",
       "15                            -  \n",
       "16                            -  \n",
       "17       (29997179479991009991)  \n",
       "18                            -  \n",
       "19                            -  \n",
       "20                            -  \n",
       "21                            -  \n",
       "22                            -  \n",
       "23                            -  \n",
       "24                            -  \n",
       "25                            -  \n",
       "26   (2999717947999115999857875  \n",
       "27                            -  \n",
       "28   2999717947999115999857875)  \n",
       "29                            -  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 20:02:39,022@18696 INFO 2321861685(22):<module>|: Amount of mentions in this topic: 7296\n",
      "2022-09-28 20:02:39,023@18696 INFO 2321861685(24):<module>|: brackets '(' , ')' : 7298, 7298\n"
     ]
    }
   ],
   "source": [
    "outputdoc_str = \"\"\n",
    "for (topic_local), topic_df in conll_df.groupby(by=[TOPIC_SUBTOPIC]):\n",
    "    outputdoc_str += f'#begin document ({topic_local}); part 000\\n'\n",
    "\n",
    "    for (sent_id_local), sent_df in topic_df.groupby(by=[SENT_ID], sort=[SENT_ID]):\n",
    "        np.savetxt(os.path.join(GVC_PARSING_FOLDER, \"tmp.txt\"), sent_df.values, fmt='%s', delimiter=\"\\t\",\n",
    "                    encoding=\"utf-8\")\n",
    "        with open(os.path.join(GVC_PARSING_FOLDER, \"tmp.txt\"), \"r\", encoding=\"utf8\") as file:\n",
    "            saved_lines = file.read()\n",
    "        outputdoc_str += saved_lines + \"\\n\"\n",
    "\n",
    "    outputdoc_str += \"#end document\\n\"\n",
    "final_output_str += outputdoc_str\n",
    "\n",
    "# Check if the brackets ( ) are correct\n",
    "try:\n",
    "    brackets_1 = 0\n",
    "    brackets_2 = 0\n",
    "    for i, row in conll_df.iterrows():  # only count brackets in reference column (exclude token text)\n",
    "        brackets_1 += str(row[REFERENCE]).count(\"(\")\n",
    "        brackets_2 += str(row[REFERENCE]).count(\")\")\n",
    "    LOGGER.info(\n",
    "        f\"Amount of mentions in this topic: {str(len(entity_mentions_local))}\")\n",
    "    LOGGER.info(f\"brackets '(' , ')' : {str(brackets_1)}, {str(brackets_2)}\")\n",
    "    assert brackets_1 == brackets_2\n",
    "except AssertionError:\n",
    "    print(final_output_str)\n",
    "    LOGGER.warning(f'Number of opening and closing brackets in conll does not match!')\n",
    "    conll_df.to_csv(os.path.join(out_path, CONLL_CSV))\n",
    "    with open(os.path.join(out_path, 'gvc.conll'), \"w\", encoding='utf-8') as file:\n",
    "        file.write(final_output_str)\n",
    "    # sys.exit()\n",
    "\n",
    "conll_df.to_csv(os.path.join(out_path, CONLL_CSV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(out_path, 'gvc.conll'), \"w\", encoding='utf-8') as file:\n",
    "    file.write(final_output_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('cdcr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "b7cf66addd738740e096863238041d528e5bd20d9f9be802df92f68260e86ac8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
